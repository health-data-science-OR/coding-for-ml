{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de72e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c9d396",
   "metadata": {},
   "source": [
    "# Statistical procedures\n",
    "\n",
    "A substantial proportion of real world applications in computational modelling require statistical procedures. `numpy` provides a wide variety of efficient statistical functions for you to employ on an array.  This section will explore the (simple and) commonly used functions that you can embed into your models and use for practical statistical analysis.  At the end of the section I'll tell a cautionary tale about blind use of `numpy` statistical procedures versus using a well design algorithm.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b> We will explore statistical programming for health data science in a lot more detail in Part 2 using `pandas` and other important libraries.  It is well worth learning `numpy` capabilities, however, as converting from a `np.ndarray` to a `pandas.DataFrame` during a computational procedure can be expensive. </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99066a8",
   "metadata": {},
   "source": [
    "## Simple data analysis example.\n",
    "\n",
    "### ED attendance data\n",
    "\n",
    "We will first use data held in the `minor_illness_ed_attends.csv`.  This is a synthetic time series dataset reporting the number of patients registered at GP surgery who attend ED each week.  The data are standardised to 10k of registered patients.\n",
    "\n",
    "#### Loading the dataset\n",
    "\n",
    "Let's first open the data and then construct some summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74719a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74,)\n"
     ]
    }
   ],
   "source": [
    "file_name = 'data/minor_illness_ed_attends.csv'\n",
    "ed_data = np.loadtxt(file_name, skiprows=1, delimiter=',')\n",
    "print(ed_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a70767",
   "metadata": {},
   "source": [
    "Here's a peak the first 5 elements in `ed_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c178efe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.11927795, 3.49057545, 3.98922908, 2.36860477, 3.24124863])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b0a1a",
   "metadata": {},
   "source": [
    "#### Calculate summary statistics\n",
    "\n",
    "`numpy` makes it easy to calculate means, stdev and other summary statistics of an `ndarray`.  This typically have intuitive names which you can often guess. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f33eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.919482262743243\n",
      "0.7060975938853894\n"
     ]
    }
   ],
   "source": [
    "print(ed_data.mean())\n",
    "print(ed_data.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cfbc17",
   "metadata": {},
   "source": [
    "But it is always worth reading the docs.  For example to calculate an unbiased estimate of the sample standard deviation of a data set of length $n$ we should divide by $n - 1$.  In `numpy` we do this using the `ddof` (degree's of freedom) parameter of `std`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ede084d",
   "metadata": {},
   "source": [
    "Here we will create a class to act as a convienient container for a dataset.  For convenience, we will override the `__str__` method so that we can easily print a summary of the dataset to the screen when calling `print`.\n",
    "\n",
    "The class will make use of the following `numpy` statistical procedures:\n",
    "\n",
    "* `min()` and `max()` to return the minimum and maximum value in array respectively\n",
    "* `np.percentile()` to calculate a percentile - here the median and upper / lower quartiles. \n",
    "* `np.histogram()` that bins data points and reports the frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46c2c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttendanceSummary:\n",
    "    '''\n",
    "    Simple container class Hold mean, stdev and 5/95 percentiles of ed data\n",
    "    '''\n",
    "    def __init__(self, data, name=None, decimal_places=2):\n",
    "        \"\"\"\n",
    "        Contructor method.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray \n",
    "            Vector containing data to analyse.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.dp = decimal_places\n",
    "        self.n = len(data)\n",
    "        self.mean = data.mean()\n",
    "        self.std = data.std(ddof=1)\n",
    "        self.min_attends = data.min()\n",
    "        self.max_attends = data.max() \n",
    "        \n",
    "        # percentiles: note that this is a np. call\n",
    "        self.lower_quartile = np.percentile(data, 25)\n",
    "        self.median = np.percentile(data, 50)\n",
    "        self.upper_quartile = np.percentile(data, 75)\n",
    "        self.iqr = self.upper_quartile - self.lower_quartile\n",
    "        \n",
    "        # frequency histogram (automatically binned)\n",
    "        self.freq, self.bins = np.histogram(data, density=False)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        to_print = f'\\nDataset: {self.name}' \\\n",
    "             + f'\\nMean:\\t{self.mean:.{self.dp}f}' \\\n",
    "             + f'\\nStdev:\\t{self.std:.{self.dp}f}' \\\n",
    "             + f'\\nMin:\\t{self.min_attends:.{self.dp}f}' \\\n",
    "             + f'\\nMax:\\t{self.max_attends:.{self.dp}f}' \\\n",
    "             + f'\\nMedian:\\t{self.median:.{self.dp}f}' \\\n",
    "             + f'\\nIQR:\\t{self.iqr:.{self.dp}f}' \\\n",
    "             + f'\\nn:\\t{self.n}'\n",
    "                    \n",
    "        return to_print\n",
    "    \n",
    "    def frequency_histogram(self):\n",
    "        print('x\\tf(x)')\n",
    "        for f, b in zip(self.freq, self.bins):\n",
    "            print(f'{b:.{self.dp}f}\\t{f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef6e8197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Dataset: ED\n",
       "Mean:\t2.92\n",
       "Stdev:\t0.71\n",
       "Min:\t1.62\n",
       "Max:\t5.11\n",
       "Median:\t2.87\n",
       "IQR:\t1.09\n",
       "n:\t74"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = AttendanceSummary(ed_data, name=\"ED\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e618648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\tf(x)\n",
      "1.62\t5\n",
      "1.97\t9\n",
      "2.32\t14\n",
      "2.67\t16\n",
      "3.02\t11\n",
      "3.37\t6\n",
      "3.71\t10\n",
      "4.06\t2\n",
      "4.41\t0\n",
      "4.76\t1\n"
     ]
    }
   ],
   "source": [
    "x.frequency_histogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c10ae",
   "metadata": {},
   "source": [
    "Its not necessary to always use a class as I here, but it is useful if for example you want to compare samples or in the case of a time series period.\n",
    "\n",
    "In our ED data let's assume an intervention was put in place in week 10: GP surgeries were opened to patients over weekends. Let's summarise the results using descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af10cf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: before\n",
      "Mean:\t3.12\n",
      "Stdev:\t0.59\n",
      "Min:\t2.12\n",
      "Max:\t3.99\n",
      "Median:\t3.18\n",
      "IQR:\t0.81\n",
      "n:\t10\n",
      "\n",
      "Dataset: after\n",
      "Mean:\t2.89\n",
      "Stdev:\t0.73\n",
      "Min:\t1.62\n",
      "Max:\t5.11\n",
      "Median:\t2.87\n",
      "IQR:\t0.90\n",
      "n:\t64\n"
     ]
    }
   ],
   "source": [
    "week = 10  # the week the intervention begins\n",
    "before = AttendanceSummary(ed_data[:week], 'before')\n",
    "after = AttendanceSummary(ed_data[week:], 'after')\n",
    "print(before)\n",
    "print(after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ab96df",
   "metadata": {},
   "source": [
    "## Statistical procedures and n-dimensional arrays\n",
    "\n",
    "Generating statistics for matricies and n-dimensional arrays works in a similar manner to vectors.  We can also go a bit further and calculate statistics across axes in the array; for example, rows and columns in a matrix. To do this you need to specify an integer **axis** in the call to the statistical function e.g. `mean(axis=0)`\n",
    "\n",
    "### Example 1: mean of matrix columns and rows\n",
    "\n",
    "Given `matrix` calculate the mean of the columns and rows.  We will first generate these manually using slices so that you can see it is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d19573fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.array([[7.7, 4.3, 8.5],\n",
    "                   [6.9, 0.9, 9.7],\n",
    "                   [7.6, 7.8, 1.2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21128761",
   "metadata": {},
   "source": [
    "To calculate the mean of the first column we need to slice all rows and the column at index 0. As a reminder we would do the following:\n",
    "\n",
    "> Try changing the value of 0 to 1 or 2 to get the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5916c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.7, 6.9, 7.6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec0d5eb",
   "metadata": {},
   "source": [
    "This is a vector and hence calculating the mean is as we have already seen \n",
    "\n",
    "> here I also call `.round(1)` to also limit displayed the result to 1 decimal place.  You could also use a format str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a27a5cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[:,0].mean().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b63d2db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3\n",
      "6.5\n"
     ]
    }
   ],
   "source": [
    "print(matrix[:,1].mean().round(1))\n",
    "print(matrix[:,2].mean().round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7820f93",
   "metadata": {},
   "source": [
    "This is a common operation so we can instead specify an axis.  In a matrix we use axis 0 for columns and 1 for rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e566e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.4, 4.3, 6.5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.mean(axis=0).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c807fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.8, 5.8, 5.5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.mean(axis=1).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ed316",
   "metadata": {},
   "source": [
    "### Example 2: Statistical operations on a larger dataset\n",
    "\n",
    "Let's open a more realistic tabular dataset that you would use in a real health data science study.  For this well download the Wisconsin breast cancer dataset from Github, create a `np.ndarray` and summarise the features.\n",
    "\n",
    "To download the file we will use the `requests` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "153b69fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "823968f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading => successful\n"
     ]
    }
   ],
   "source": [
    "#download successful\n",
    "RESPONSE_SUCCESS = 200\n",
    "FILE_NAME = 'data/wisconsin.csv'\n",
    "\n",
    "# note: github raw url needed for download\n",
    "DATA_URL = \"https://raw.githubusercontent.com/health-data-science-OR/\" \\\n",
    "            + \"hpdm139-datasets/main/wisconsin.csv\"\n",
    "\n",
    "# download the file...(only needs to be done once!).\n",
    "print('downloading =>', end=' ')\n",
    "response = requests.get(DATA_URL)\n",
    "\n",
    "if response.status_code == RESPONSE_SUCCESS:\n",
    "    print('successful')\n",
    "    \n",
    "    # response.content is a in bytes\n",
    "    # write to file to read into numpy.\n",
    "    with open(FILE_NAME, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "else:\n",
    "    print('file not found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6c2f33",
   "metadata": {},
   "source": [
    "Before we attempt to load the `numpy` array from let's do some due diligence and check for headers and any non-numeric column data types. We need to do this to avoid any issues when loading the data.  We will go a bit old school here and use the `csv` library to help us with the inspection.\n",
    "\n",
    "> This isn't a big dataset so we could just inspect it in a CSV viewer in JupyterLab (or in a free and open spreadsheet package like LibreOffice Calc).  But I'd encourage you to keep your complete workflow documented in health data science. In some cases the dataset will also be far to big for memory and you'll standard viewers won't be able to cope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d65a5a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'id', 'diagnosis', 'radius_mean', 'texture_mean']\n",
      "No. fields: 33\n",
      "Fields with non-numeric data: [2]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(FILE_NAME, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    header_row = next(reader)\n",
    "    field_count = len(header_row)\n",
    "    # peak at the header_row\n",
    "    print(header_row[:5])\n",
    "    # record names\n",
    "    header_names = [header for header in header_row]\n",
    "    print(f'No. fields: {field_count}')\n",
    "    \n",
    "    # peak at first data row\n",
    "    first_data_row = next(reader)\n",
    "    \n",
    "    non_numeric_fields = []\n",
    "    # catch any fields that contain non numeric data\n",
    "    for field_index in range(len(first_data_row)):\n",
    "        try:\n",
    "            field = float(first_data_row[field_index])\n",
    "        except:\n",
    "            non_numeric_fields.append(field_index)\n",
    "    \n",
    "    # print out non-numeric fields\n",
    "    print(f'Fields with non-numeric data: {non_numeric_fields}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e592404",
   "metadata": {},
   "source": [
    "We can conclude that the first row in the file is indeed a header row - we'll skip that on the array import.  We will also drop the fields denoted ' ' and 'id'. The field at index 2: `diagnosis` contains non-numeric data.  This is actually the target field in the dataset ('M' for malignant and 'B' for benign). We will deal with the target seperately here.  \n",
    "\n",
    "The first thing we will do is create a list of the field indexes containing the numeric data to read in, using boolean indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "825e3c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29 30 31 32]\n"
     ]
    }
   ],
   "source": [
    "field_indexes = np.arange(field_count)\n",
    "fields_included = field_indexes[~np.isin(field_indexes, non_numeric_fields)]\n",
    "fields_included = fields_included[2:]\n",
    "header_names = np.array(header_names)[fields_included]\n",
    "print(fields_included)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697568c",
   "metadata": {},
   "source": [
    "Then its just a case of loading the correct rows and columns into an array.  Here we will use `np.genfromtxt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64f7977a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n"
     ]
    }
   ],
   "source": [
    "wisconsin = np.genfromtxt(FILE_NAME, skip_header=1, usecols=fields_included, delimiter=',')\n",
    "print(wisconsin.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69883a1b",
   "metadata": {},
   "source": [
    "We now have a clean dataset that we can summarise. All fields are quantatitive so this is straightforward.  The class `FeatureSummary` below helps us visualise the results of each feature (column).  Call the `.show()` method to see a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2e8a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSummary:\n",
    "    '''\n",
    "    Simple container class to hold and display a descriptive summary\n",
    "    of features in a dataset\n",
    "    '''\n",
    "    def __init__(self, data, headers, name=None, decimal_places=2):\n",
    "        \"\"\"\n",
    "        Contructor method.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray \n",
    "            Vector containing data to analyse.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.headers = headers\n",
    "        # these help with the summary layout.\n",
    "        self.max_header_padding_len = max(len(x) for x in headers)\n",
    "        self.header_padding = ''.rjust(self.max_header_padding_len)\n",
    "        print(self.header_padding)\n",
    "        self.dp = decimal_places\n",
    "        self.n = len(data)\n",
    "        self.mean = data.mean(axis=0)\n",
    "        self.std = data.std(ddof=1, axis=0)\n",
    "        self.min = data.min(axis=0)\n",
    "        self.max = data.max(axis=0) \n",
    "        self.range = self.max - self.min\n",
    "        \n",
    "        # percentiles: note that this is a np. call\n",
    "        self.lower_quartile = np.percentile(data, 25, axis=0)\n",
    "        self.median = np.percentile(data, 50, axis=0)\n",
    "        self.upper_quartile = np.percentile(data, 75, axis=0)\n",
    "        self.iqr = self.upper_quartile - self.lower_quartile\n",
    "    \n",
    "    def show(self):\n",
    "        print(self.name)\n",
    "        header_row = f\"{self.header_padding}\\tMean\\tStd\\tMedian\\tIQR\\tRange\"\n",
    "        print(header_row)\n",
    "        line = ['-'] * int(len(header_row) * 1.3)\n",
    "        print(''.join(line))\n",
    "        for i in range(len(self.headers)):\n",
    "            row_padding_len = self.max_header_padding_len - len(self.headers[i])\n",
    "            row_padding = ''.rjust(row_padding_len)\n",
    "            field = f'{self.headers[i]}{row_padding}\\t{self.mean[i]:.{self.dp}f}' \\\n",
    "                    + f'\\t{self.std[i]:.{self.dp}f}' \\\n",
    "                    + f'\\t{self.median[i]:.{self.dp}f}' \\\n",
    "                    + f'\\t{self.iqr[i]:.{self.dp}f}' \\\n",
    "                    + f'\\t{self.range[i]:.{self.dp}f}'\n",
    "            print(field) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f70f7f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       \n",
      "Wisconson Breast Cancer Dataset.\n",
      "                       \tMean\tStd\tMedian\tIQR\tRange\n",
      "---------------------------------------------------------------\n",
      "radius_mean            \t14.13\t3.52\t13.37\t4.08\t21.13\n",
      "texture_mean           \t19.29\t4.30\t18.84\t5.63\t29.57\n",
      "perimeter_mean         \t91.97\t24.30\t86.24\t28.93\t144.71\n",
      "area_mean              \t654.89\t351.91\t551.10\t362.40\t2357.50\n",
      "smoothness_mean        \t0.10\t0.01\t0.10\t0.02\t0.11\n",
      "compactness_mean       \t0.10\t0.05\t0.09\t0.07\t0.33\n",
      "concavity_mean         \t0.09\t0.08\t0.06\t0.10\t0.43\n",
      "concave points_mean    \t0.05\t0.04\t0.03\t0.05\t0.20\n",
      "symmetry_mean          \t0.18\t0.03\t0.18\t0.03\t0.20\n",
      "fractal_dimension_mean \t0.06\t0.01\t0.06\t0.01\t0.05\n",
      "radius_se              \t0.41\t0.28\t0.32\t0.25\t2.76\n",
      "texture_se             \t1.22\t0.55\t1.11\t0.64\t4.52\n",
      "perimeter_se           \t2.87\t2.02\t2.29\t1.75\t21.22\n",
      "area_se                \t40.34\t45.49\t24.53\t27.34\t535.40\n",
      "smoothness_se          \t0.01\t0.00\t0.01\t0.00\t0.03\n",
      "compactness_se         \t0.03\t0.02\t0.02\t0.02\t0.13\n",
      "concavity_se           \t0.03\t0.03\t0.03\t0.03\t0.40\n",
      "concave points_se      \t0.01\t0.01\t0.01\t0.01\t0.05\n",
      "symmetry_se            \t0.02\t0.01\t0.02\t0.01\t0.07\n",
      "fractal_dimension_se   \t0.00\t0.00\t0.00\t0.00\t0.03\n",
      "radius_worst           \t16.27\t4.83\t14.97\t5.78\t28.11\n",
      "texture_worst          \t25.68\t6.15\t25.41\t8.64\t37.52\n",
      "perimeter_worst        \t107.26\t33.60\t97.66\t41.29\t200.79\n",
      "area_worst             \t880.58\t569.36\t686.50\t568.70\t4068.80\n",
      "smoothness_worst       \t0.13\t0.02\t0.13\t0.03\t0.15\n",
      "compactness_worst      \t0.25\t0.16\t0.21\t0.19\t1.03\n",
      "concavity_worst        \t0.27\t0.21\t0.23\t0.27\t1.25\n",
      "concave points_worst   \t0.11\t0.07\t0.10\t0.10\t0.29\n",
      "symmetry_worst         \t0.29\t0.06\t0.28\t0.07\t0.51\n",
      "fractal_dimension_worst\t0.08\t0.02\t0.08\t0.02\t0.15\n"
     ]
    }
   ],
   "source": [
    "x = FeatureSummary(wisconsin, header_names, name='Wisconson Breast Cancer Dataset.')\n",
    "x.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ada2c",
   "metadata": {},
   "source": [
    "## A note of caution: an example of when `numpy` is less efficient for stats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e62bcf",
   "metadata": {},
   "source": [
    "In many computational modelling procedures you will need an updated estimate of statistics as the code executes. For example, you may need to track a mean or a standard deviation of a performance measure in a multi-stage algorithm or as a simulation model of a healthcare system executes. \n",
    "\n",
    "As we have seen `numpy` provides highly efficient functions for calculating a mean or standard deviation based on data held in an array.  I'm always tempted to make use of these built in procedures. They are indeed fast and incredibly easy to use.  The downside is that you waste computation via repeated iteration over an array.  The other option, that requires more careful thought (due to floating point issues), is a running estimate of your statistics.  In general, I've implemented such procedures  in standard python.  Let's look at an example where we compare recalculation using a `numpy` function with a running (sometimes called an 'online') calculation of the mean and standard deviation in standard python.\n",
    "\n",
    "We will first refactor `AttendanceSummary` to an `OnlineSummary` class to include an `update()` function.  It will accept a `np.ndarray` that recalculates the **sample** mean and standard deviation using a `numpy` on the full data set.  The function `test_complete_recalculation` iteratively calls `update` using more data each time.  For simplicities sake we will reuse the data contained within `ed_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa700a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineSummary:\n",
    "    \n",
    "    def __init__(self, data=None, decimal_places=2):\n",
    "        \"\"\"\n",
    "        Track online statistics of mean and standard deviation.\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        data: np.ndarray, optional (default = None) \n",
    "            Contains an initial data sample.\n",
    "            \n",
    "        decimal_places: int, optional (default=2)\n",
    "            Summary decimal places.\n",
    "        \"\"\"\n",
    "        if isinstance(data, np.ndarray):\n",
    "            self.n = len(data)\n",
    "            self.mean = data.mean()\n",
    "            self.std = data.std(ddof=1)\n",
    "        else:\n",
    "            self.n = 0\n",
    "            self.mean = None\n",
    "            self.std = None\n",
    "            \n",
    "        self.dp = decimal_places\n",
    "        \n",
    "    def update(self, data):\n",
    "        '''\n",
    "        Update the mean and standard deviation using complete recalculation.\n",
    "        \n",
    "        Params:\n",
    "        ------\n",
    "        data: np.ndarray\n",
    "            Vector of data\n",
    "        '''\n",
    "        self.n = len(data)\n",
    "        \n",
    "        # update the mean and std. Easy!\n",
    "        self.mean = data.mean()\n",
    "        self.std = data.std(ddof=1)\n",
    "        \n",
    "    \n",
    "    def __str__(self):\n",
    "        to_print = f'Mean:\\t{self.mean:.{self.dp}f}' \\\n",
    "             + f'\\nStdev:\\t{self.std:.{self.dp}f}' \\\n",
    "             + f'\\nn:\\t{self.n}' \\\n",
    "        \n",
    "        return to_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aff0731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_complete_recalculation(data, start=2):\n",
    "    summary = OnlineSummary(data[:start])\n",
    "\n",
    "    for i in range(start, len(data)+1):\n",
    "        summary.update(data[:i])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a2b46e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\t2.92\n",
      "Stdev:\t0.71\n",
      "n:\t74\n"
     ]
    }
   ],
   "source": [
    "summary = test_complete_recalculation(ed_data)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1e2994b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15 ms ± 45.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit summary = test_complete_recalculation(ed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3694bdec",
   "metadata": {},
   "source": [
    "You should find the `numpy` implementation fairly efficient clocking in at around 1.5ms on average. But can we do better in standard python by computing an online mean and standard deviation?\n",
    "\n",
    "To do this we will use **Welford's algorithm** for computing a running sample mean and standard deviation.  This is a robust, accuate and old(ish) approach (1960s) that I first read about in Donald Knuth's *art of computer programming vol 2.* (just to be clear I learnt how to do this in 2008 not 1960!).  To implement it we need to refactor `update`.  Note that we will need a fair bit more code than our simple `numpy` solution.\n",
    "\n",
    "The algorithm is given in a recursive format.  For our purposes here, you can just think of that as tracking the mean and standard deviation as attributes of a class that we iteratively update with a new $x$. \n",
    "\n",
    "The first thing you need to do is handle the first observation encountered. \n",
    "\n",
    "$$M_1 = x_1$$\n",
    "$$S_1 = 0$$\n",
    "\n",
    "Then on each subsequent call you update $M$ and $S$ making use of the previous values.  Note that $M$ has a relatively simple interpretation: its the sample mean. However, $S$ is not the standard deviation.  Its actually the sum of squares of differences from the current mean.  We will look at how to update that first and then I'll show you the equation for converting to the standard deviation.\n",
    "\n",
    "$$M_n = M_{n-1} + \\dfrac{x_n - M_{n-1}}{n}$$\n",
    "\n",
    "\n",
    "$$S_n = S_{n-1} + \\left[(x_n - M_{n-1}) \\times (x_n - M_n)\\right]$$\n",
    "\n",
    "If the equations are confusing you can think of $M_n$ as the `updated_mean` and $M_{n-1}$ as the `previous_mean`.\n",
    "\n",
    "Once the update is complete it is then relatively trivial to calculate the standard deviation $\\sigma_n$.  Note that we don't necessarily need to track the standard deviation just $S_n$.  We can inexpensively calculate $\\sigma_n$ when it is needed.  \n",
    "\n",
    "$$\\sigma_n = \\sqrt{\\dfrac{S_n}{n-1}}$$\n",
    "\n",
    "The code listing below modifies `OnlineSummary` to make use of Welford's algorithm. Note that `std` is now a property that calculates the standard deviation on the fly using $S_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d70bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineSummary:\n",
    "    \n",
    "    def __init__(self, data=None, decimal_places=2):\n",
    "        \"\"\"\n",
    "        Returns mean, stdev and 5/95 percentiles of ed data\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        data: np.ndarray, optional (default = None) \n",
    "            Contains an initial data sample.\n",
    "            \n",
    "        decimal_places: int, optional (default=2)\n",
    "            Summary decimal places.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n = 0\n",
    "        self.mean = None\n",
    "        self._sq = None\n",
    "        \n",
    "        if isinstance(data, np.ndarray):\n",
    "            for x in data:\n",
    "                self.update(x)\n",
    "            \n",
    "        self.dp = decimal_places\n",
    "    \n",
    "    @property\n",
    "    def variance(self):\n",
    "        return self._sq / (self.n - 1)\n",
    "    \n",
    "    @property\n",
    "    def std(self):\n",
    "        return np.sqrt(self.variance)\n",
    "    \n",
    "    def update(self, x):\n",
    "        '''\n",
    "        Running update of mean and variance implemented using Welford's \n",
    "        algorithm (1962).\n",
    "        \n",
    "        See Knuth. D `The Art of Computer Programming` Vol 2. 2nd ed. Page 216.\n",
    "        \n",
    "        Params:\n",
    "        ------\n",
    "        x: float\n",
    "            A new observation\n",
    "        '''\n",
    "        self.n += 1\n",
    "        \n",
    "        # we need to do more work ourselves for online stats!\n",
    "        \n",
    "        # init values\n",
    "        if self.n == 1:\n",
    "            self.mean = x\n",
    "            self._sq = 0\n",
    "        else:\n",
    "            # compute the updated mean\n",
    "            updated_mean = self.mean + ((x - self.mean) / self.n)\n",
    "        \n",
    "            # update the sum of squares \n",
    "            self._sq += (x - self.mean) * (x - updated_mean)\n",
    "            \n",
    "            # update the tracked mean\n",
    "            self.mean = updated_mean\n",
    "    \n",
    "    def __str__(self):\n",
    "        to_print = f'Mean:\\t{self.mean:.{self.dp}f}' \\\n",
    "             + f'\\nStdev:\\t{self.std:.{self.dp}f}' \\\n",
    "             + f'\\nn:\\t{self.n}' \\\n",
    "        \n",
    "        return to_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04f9121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_online_calculation(data, start=1):\n",
    "    summary = OnlineSummary()\n",
    "\n",
    "    for observation in data:\n",
    "        summary.update(observation)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c89ba4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\t2.92\n",
      "Stdev:\t0.71\n",
      "n:\t74\n"
     ]
    }
   ],
   "source": [
    "summary = test_online_calculation(ed_data)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d89a47cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 µs ± 4.59 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit summary = test_online_calculation(ed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb8301",
   "metadata": {},
   "source": [
    "Crickey nothing beats a good algorithm! You should find that you are now working in microseconds (µs) as opposed to milliseconds. 1µs = 1000ms. On my machine the `test_online_calculation` executes in ~45 µs on average while `test_complete_recalculation` takes ~1500 µs. So we are finding a speed up of ~97%. That gap will continue to grow as the number of samples $n$ increases.  The result is explained because our second implementation has a constant time for execution (and constant number of computational steps) while the time complexity of the `numpy` call depends on the size of the array. That's a lesson well worth remembering when developing code for scientific applications requiring performant code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
