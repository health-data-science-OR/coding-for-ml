{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de72e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c9d396",
   "metadata": {},
   "source": [
    "# Statistical procedures\n",
    "\n",
    "A substantial proportion of real world applications in computational modelling require statistical procedures. `numpy` provides a wide variety of efficient statistical functions for you to employ on an array.  This section will explore the (simple and) commonly used functions that you can embed into your models and use for practical statistical analysis.  \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b> We will explore statistical programming for health data science in a lot more detail in Part 2 using `pandas` and other important libraries.  It is well worth learning `numpy` capabilities, however, as converting from a `np.ndarray` to a `pandas.DataFrame` during a computational procedure can be expensive. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99066a8",
   "metadata": {},
   "source": [
    "## Simple data analysis example.\n",
    "\n",
    "### ED attendance data\n",
    "\n",
    "We will first use data held in the `minor_illness_ed_attends.csv`.  This is a synthetic time series dataset reporting the number of patients registered at GP surgery who attend ED each week.  The data are standardised to 10k of registered patients.\n",
    "\n",
    "#### Loading the dataset\n",
    "\n",
    "Let's first open the data and then construct some summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74719a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74,)\n"
     ]
    }
   ],
   "source": [
    "file_name = 'data/minor_illness_ed_attends.csv'\n",
    "ed_data = np.loadtxt(file_name, skiprows=1, delimiter=',')\n",
    "print(ed_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a70767",
   "metadata": {},
   "source": [
    "Here's a peak the first 5 elements in `ed_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c178efe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.11927795, 3.49057545, 3.98922908, 2.36860477, 3.24124863])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b0a1a",
   "metadata": {},
   "source": [
    "#### Calculate summary statistics\n",
    "\n",
    "`numpy` makes it easy to calculate means, stdev and other summary statistics of an `ndarray`.  This typically have intuitive names which you can often guess. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f33eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.919482262743243\n",
      "0.7060975938853894\n"
     ]
    }
   ],
   "source": [
    "print(ed_data.mean())\n",
    "print(ed_data.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cfbc17",
   "metadata": {},
   "source": [
    "But it is always worth reading the docs.  For example to calculate an unbiased estimate of the sample standard deviation of a data set of length $n$ we should divide by $n - 1$.  In `numpy` we do this using the `ddof` (degree's of freedom) parameter of `std`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ede084d",
   "metadata": {},
   "source": [
    "Here we will create a class to act as a convienient container for a dataset.  For convenience, we will override the `__str__` method so that we can easily print a summary of the dataset to the screen when calling `print`.\n",
    "\n",
    "The class will make use of the following `numpy` statistical procedures:\n",
    "\n",
    "* `min()` and `max()` to return the minimum and maximum value in array respectively\n",
    "* `np.percentile()` to calculate a percentile - here the median and upper / lower quartiles. \n",
    "* `np.histogram()` that bins data points and reports the frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46c2c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttendanceSummary:\n",
    "    '''\n",
    "    Simple container class Hold mean, stdev and 5/95 percentiles of ed data\n",
    "    '''\n",
    "    def __init__(self, data, name=None, decimal_places=2):\n",
    "        \"\"\"\n",
    "        Contructor method.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray \n",
    "            Vector containing data to analyse.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.dp = decimal_places\n",
    "        self.n = len(data)\n",
    "        self.mean = data.mean()\n",
    "        self.std = data.std(ddof=1)\n",
    "        self.min_attends = data.min()\n",
    "        self.max_attends = data.max() \n",
    "        \n",
    "        # percentiles: note that this is a np. call\n",
    "        self.lower_quartile = np.percentile(data, 25)\n",
    "        self.median = np.percentile(data, 50)\n",
    "        self.upper_quartile = np.percentile(data, 75)\n",
    "        self.iqr = self.upper_quartile - self.lower_quartile\n",
    "        \n",
    "        # frequency histogram (automatically binned)\n",
    "        self.freq, self.bins = np.histogram(data, density=False)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        to_print = f'\\nDataset: {self.name}' \\\n",
    "             + f'\\nMean:\\t{self.mean:.{self.dp}f}' \\\n",
    "             + f'\\nStdev:\\t{self.std:.{self.dp}f}' \\\n",
    "             + f'\\nMin:\\t{self.min_attends:.{self.dp}f}' \\\n",
    "             + f'\\nMax:\\t{self.max_attends:.{self.dp}f}' \\\n",
    "             + f'\\nMedian:\\t{self.median:.{self.dp}f}' \\\n",
    "             + f'\\nIQR:\\t{self.iqr:.{self.dp}f}' \\\n",
    "             + f'\\nn:\\t{self.n}'\n",
    "                    \n",
    "        return to_print\n",
    "    \n",
    "    def frequency_histogram(self):\n",
    "        print('x\\tf(x)')\n",
    "        for f, b in zip(self.freq, self.bins):\n",
    "            print(f'{b:.{self.dp}f}\\t{f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef6e8197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Dataset: ED\n",
       "Mean:\t2.92\n",
       "Stdev:\t0.71\n",
       "Min:\t1.62\n",
       "Max:\t5.11\n",
       "Median:\t2.87\n",
       "IQR:\t1.09\n",
       "n:\t74"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = AttendanceSummary(ed_data, name=\"ED\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e618648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\tf(x)\n",
      "1.62\t5\n",
      "1.97\t9\n",
      "2.32\t14\n",
      "2.67\t16\n",
      "3.02\t11\n",
      "3.37\t6\n",
      "3.71\t10\n",
      "4.06\t2\n",
      "4.41\t0\n",
      "4.76\t1\n"
     ]
    }
   ],
   "source": [
    "x.frequency_histogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c10ae",
   "metadata": {},
   "source": [
    "Its not necessary to always use a class as I here, but it is useful if for example you want to compare samples or in the case of a time series period.\n",
    "\n",
    "In our ED data let's assume an intervention was put in place in week 10: GP surgeries were opened to patients over weekends. Let's summarise the results using descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af10cf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: before\n",
      "Mean:\t3.12\n",
      "Stdev:\t0.59\n",
      "Min:\t2.12\n",
      "Max:\t3.99\n",
      "Median:\t3.18\n",
      "IQR:\t0.81\n",
      "n:\t10\n",
      "\n",
      "Dataset: after\n",
      "Mean:\t2.89\n",
      "Stdev:\t0.73\n",
      "Min:\t1.62\n",
      "Max:\t5.11\n",
      "Median:\t2.87\n",
      "IQR:\t0.90\n",
      "n:\t64\n"
     ]
    }
   ],
   "source": [
    "week = 10  # the week the intervention begins\n",
    "before = AttendanceSummary(ed_data[:week], 'before')\n",
    "after = AttendanceSummary(ed_data[week:], 'after')\n",
    "print(before)\n",
    "print(after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ab96df",
   "metadata": {},
   "source": [
    "## Statistical procedures and n-dimensional arrays\n",
    "\n",
    "Generating statistics for matricies and n-dimensional arrays works in a similar manner to vectors.  We can also go a bit further and calculate statistics across axes in the array; for example, rows and columns in a matrix. To do this you need to specify an integer **axis** in the call to the statistical function e.g. `mean(axis=0)`\n",
    "\n",
    "### Example 1: mean of matrix columns and rows\n",
    "\n",
    "Given `matrix` calculate the mean of the columns and rows.  We will first generate these manually using slices so that you can see it is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d19573fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.array([[7.7, 4.3, 8.5],\n",
    "                   [6.9, 0.9, 9.7],\n",
    "                   [7.6, 7.8, 1.2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21128761",
   "metadata": {},
   "source": [
    "To calculate the mean of the first column we need to slice all rows and the column at index 0. As a reminder we would do the following:\n",
    "\n",
    "> Try changing the value of 0 to 1 or 2 to get the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5916c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.7, 6.9, 7.6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec0d5eb",
   "metadata": {},
   "source": [
    "This is a vector and hence calculating the mean is as we have already seen \n",
    "\n",
    "> here I also call `.round(1)` to also limit displayed the result to 1 decimal place.  You could also use a format str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a27a5cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[:,0].mean().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b63d2db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3\n",
      "6.5\n"
     ]
    }
   ],
   "source": [
    "print(matrix[:,1].mean().round(1))\n",
    "print(matrix[:,2].mean().round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7820f93",
   "metadata": {},
   "source": [
    "This is a common operation so we can instead specify an axis.  In a matrix we use axis 0 for columns and 1 for rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e566e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.4, 4.3, 6.5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.mean(axis=0).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c807fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.8, 5.8, 5.5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.mean(axis=1).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ed316",
   "metadata": {},
   "source": [
    "### Example 2: Statistical operations on a larger dataset\n",
    "\n",
    "Let's open a more realistic tabular dataset that you would use in a real health data science study.  For this well download the Wisconsin breast cancer dataset from Github, create a `np.ndarray` and summarise the features.\n",
    "\n",
    "To download the file we will use the `requests` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "153b69fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "823968f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading => successful\n"
     ]
    }
   ],
   "source": [
    "#download successful\n",
    "RESPONSE_SUCCESS = 200\n",
    "FILE_NAME = 'data/wisconsin.csv'\n",
    "\n",
    "# note: github raw url needed for download\n",
    "DATA_URL = \"https://raw.githubusercontent.com/health-data-science-OR/\" \\\n",
    "            + \"hpdm139-datasets/main/wisconsin.csv\"\n",
    "\n",
    "# download the file...(only needs to be done once!).\n",
    "print('downloading =>', end=' ')\n",
    "response = requests.get(DATA_URL)\n",
    "\n",
    "if response.status_code == RESPONSE_SUCCESS:\n",
    "    print('successful')\n",
    "    \n",
    "    # response.content is a in bytes\n",
    "    # write to file to read into numpy.\n",
    "    with open(FILE_NAME, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "else:\n",
    "    print('file not found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6c2f33",
   "metadata": {},
   "source": [
    "Before we attempt to load the `numpy` array from let's do some due diligence and check for headers and any non-numeric column data types. We need to do this to avoid any issues when loading the data.  We will go a bit old school here and use the `csv` library to help us with the inspection.\n",
    "\n",
    "> This isn't a big dataset so we could just inspect it in a CSV viewer in JupyterLab (or in a free and open spreadsheet package like LibreOffice Calc).  But I'd encourage you to keep your complete workflow documented in health data science. In some cases the dataset will also be far to big for memory and you'll standard viewers won't be able to cope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d65a5a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'id', 'diagnosis', 'radius_mean', 'texture_mean']\n",
      "No. fields: 33\n",
      "Fields with non-numeric data: [2]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(FILE_NAME, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    header_row = next(reader)\n",
    "    field_count = len(header_row)\n",
    "    # peak at the header_row\n",
    "    print(header_row[:5])\n",
    "    # record names\n",
    "    header_names = [header for header in header_row]\n",
    "    print(f'No. fields: {field_count}')\n",
    "    \n",
    "    # peak at first data row\n",
    "    first_data_row = next(reader)\n",
    "    \n",
    "    non_numeric_fields = []\n",
    "    # catch any fields that contain non numeric data\n",
    "    for field_index in range(len(first_data_row)):\n",
    "        try:\n",
    "            field = float(first_data_row[field_index])\n",
    "        except:\n",
    "            non_numeric_fields.append(field_index)\n",
    "    \n",
    "    # print out non-numeric fields\n",
    "    print(f'Fields with non-numeric data: {non_numeric_fields}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e592404",
   "metadata": {},
   "source": [
    "We can conclude that the first row in the file is indeed a header row - we'll skip that on the array import.  We will also drop the fields denoted ' ' and 'id'. The field at index 2: `diagnosis` contains non-numeric data.  This is actually the target field in the dataset ('M' for malignant and 'B' for benign). We will deal with the target seperately here.  \n",
    "\n",
    "The first thing we will do is create a list of the field indexes containing the numeric data to read in, using boolean indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "825e3c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29 30 31 32]\n"
     ]
    }
   ],
   "source": [
    "field_indexes = np.arange(field_count)\n",
    "fields_included = field_indexes[~np.isin(field_indexes, non_numeric_fields)]\n",
    "fields_included = fields_included[2:]\n",
    "header_names = np.array(header_names)[fields_included]\n",
    "print(fields_included)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697568c",
   "metadata": {},
   "source": [
    "Then its just a case of loading the correct rows and columns into an array.  Here we will use `np.genfromtxt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64f7977a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n"
     ]
    }
   ],
   "source": [
    "wisconsin = np.genfromtxt(FILE_NAME, skip_header=1, usecols=fields_included, delimiter=',')\n",
    "print(wisconsin.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69883a1b",
   "metadata": {},
   "source": [
    "We now have a clean dataset that we can summarise. All fields are quantatitive so this is straightforward.  The class `FeatureSummary` below helps us visualise the results of each feature (column).  Call the `.show()` method to see a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2e8a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSummary:\n",
    "    '''\n",
    "    Simple container class to hold and display a descriptive summary\n",
    "    of features in a dataset\n",
    "    '''\n",
    "    def __init__(self, data, headers, name=None, decimal_places=2):\n",
    "        \"\"\"\n",
    "        Contructor method.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray \n",
    "            Vector containing data to analyse.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.headers = headers\n",
    "        # these help with the summary layout.\n",
    "        self.max_header_padding_len = max(len(x) for x in headers)\n",
    "        self.header_padding = ''.rjust(self.max_header_padding_len)\n",
    "        print(self.header_padding)\n",
    "        self.dp = decimal_places\n",
    "        self.n = len(data)\n",
    "        self.mean = data.mean(axis=0)\n",
    "        self.std = data.std(ddof=1, axis=0)\n",
    "        self.min = data.min(axis=0)\n",
    "        self.max = data.max(axis=0) \n",
    "        self.range = self.max - self.min\n",
    "        \n",
    "        # percentiles: note that this is a np. call\n",
    "        self.lower_quartile = np.percentile(data, 25, axis=0)\n",
    "        self.median = np.percentile(data, 50, axis=0)\n",
    "        self.upper_quartile = np.percentile(data, 75, axis=0)\n",
    "        self.iqr = self.upper_quartile - self.lower_quartile\n",
    "    \n",
    "    def show(self):\n",
    "        print(self.name)\n",
    "        header_row = f\"{self.header_padding}\\tMean\\tStd\\tMedian\\tIQR\\tRange\"\n",
    "        print(header_row)\n",
    "        line = ['-'] * int(len(header_row) * 1.3)\n",
    "        print(''.join(line))\n",
    "        for i in range(len(self.headers)):\n",
    "            row_padding_len = self.max_header_padding_len - len(self.headers[i])\n",
    "            row_padding = ''.rjust(row_padding_len)\n",
    "            field = f'{self.headers[i]}{row_padding}\\t{self.mean[i]:.{self.dp}f}' \\\n",
    "                    + f'\\t{self.std[i]:.{self.dp}f}' \\\n",
    "                    + f'\\t{self.median[i]:.{self.dp}f}' \\\n",
    "                    + f'\\t{self.iqr[i]:.{self.dp}f}' \\\n",
    "                    + f'\\t{self.range[i]:.{self.dp}f}'\n",
    "            print(field) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f70f7f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       \n",
      "Wisconson Breast Cancer Dataset.\n",
      "                       \tMean\tStd\tMedian\tIQR\tRange\n",
      "---------------------------------------------------------------\n",
      "radius_mean            \t14.13\t3.52\t13.37\t4.08\t21.13\n",
      "texture_mean           \t19.29\t4.30\t18.84\t5.63\t29.57\n",
      "perimeter_mean         \t91.97\t24.30\t86.24\t28.93\t144.71\n",
      "area_mean              \t654.89\t351.91\t551.10\t362.40\t2357.50\n",
      "smoothness_mean        \t0.10\t0.01\t0.10\t0.02\t0.11\n",
      "compactness_mean       \t0.10\t0.05\t0.09\t0.07\t0.33\n",
      "concavity_mean         \t0.09\t0.08\t0.06\t0.10\t0.43\n",
      "concave points_mean    \t0.05\t0.04\t0.03\t0.05\t0.20\n",
      "symmetry_mean          \t0.18\t0.03\t0.18\t0.03\t0.20\n",
      "fractal_dimension_mean \t0.06\t0.01\t0.06\t0.01\t0.05\n",
      "radius_se              \t0.41\t0.28\t0.32\t0.25\t2.76\n",
      "texture_se             \t1.22\t0.55\t1.11\t0.64\t4.52\n",
      "perimeter_se           \t2.87\t2.02\t2.29\t1.75\t21.22\n",
      "area_se                \t40.34\t45.49\t24.53\t27.34\t535.40\n",
      "smoothness_se          \t0.01\t0.00\t0.01\t0.00\t0.03\n",
      "compactness_se         \t0.03\t0.02\t0.02\t0.02\t0.13\n",
      "concavity_se           \t0.03\t0.03\t0.03\t0.03\t0.40\n",
      "concave points_se      \t0.01\t0.01\t0.01\t0.01\t0.05\n",
      "symmetry_se            \t0.02\t0.01\t0.02\t0.01\t0.07\n",
      "fractal_dimension_se   \t0.00\t0.00\t0.00\t0.00\t0.03\n",
      "radius_worst           \t16.27\t4.83\t14.97\t5.78\t28.11\n",
      "texture_worst          \t25.68\t6.15\t25.41\t8.64\t37.52\n",
      "perimeter_worst        \t107.26\t33.60\t97.66\t41.29\t200.79\n",
      "area_worst             \t880.58\t569.36\t686.50\t568.70\t4068.80\n",
      "smoothness_worst       \t0.13\t0.02\t0.13\t0.03\t0.15\n",
      "compactness_worst      \t0.25\t0.16\t0.21\t0.19\t1.03\n",
      "concavity_worst        \t0.27\t0.21\t0.23\t0.27\t1.25\n",
      "concave points_worst   \t0.11\t0.07\t0.10\t0.10\t0.29\n",
      "symmetry_worst         \t0.29\t0.06\t0.28\t0.07\t0.51\n",
      "fractal_dimension_worst\t0.08\t0.02\t0.08\t0.02\t0.15\n"
     ]
    }
   ],
   "source": [
    "x = FeatureSummary(wisconsin, header_names, name='Wisconson Breast Cancer Dataset.')\n",
    "x.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
