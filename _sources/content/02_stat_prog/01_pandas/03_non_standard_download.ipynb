{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v-Pp51PT69x"
   },
   "source": [
    "# Case study: a large remote dataset\n",
    "\n",
    "> Here we will download and extract a remote NHS Synthetic Data that uses a non-standard compression.\n",
    "\n",
    "NHS England publishes a fantastic [synthetic dataset](https://data.england.nhs.uk/dataset/a-e-synthetic-data) for Accident and Emergency departments in England.  This is a relatively large clean dataset that contains over 65M rows.\n",
    "\n",
    "**In this worked example you will learn:**\n",
    "* How to download a remote data source \n",
    "* How to use third party libraries to decompress a .7z archive.\n",
    "* How to reduce the size of a large `pandas.DataFrame` in memory.\n",
    "\n",
    "---\n",
    "## Complications in use of the full dataset.\n",
    "\n",
    "Even though this dataset is sanitised and pre-cleaned there are two complications if you are new to data science.\n",
    "\n",
    "### Compression format\n",
    "\n",
    "* The public URL for this dataset provides access to the data in **7zip** format (.7z). The .7z format offers a good compression ratio (4.5GB compresses to 630MB), but at the cost of using a non-standard (and to be honest Microsoft Windows OS centric) format.  \n",
    "* Ideally all machine learning studies have a reproducible pipeline from raw data at source through final analysis and results. At the time of writing `pandas` does not natively deal with .7z and a third party dependency is needed or it must be extracted outside of the python environment.  \n",
    "* `pandas` can handle other formats; for example, **.zip** and **.tar**\n",
    "\n",
    "> Here we will focus on how to decompress .7z within python using a third party dependency.\n",
    "\n",
    "### Memory requirements\n",
    "\n",
    "* This notebook demonstrates how to download the NHS England hosted dataset. This is a large dataset to download: 630MB compressed and 4.5GB uncompressed.  \n",
    "* If read directly into `pandas` without considering datatypes of each column the size in memory is 8.8GB.  Depending on your machine this may be extremely problematic (it is even problematic using the generous memory provided by Google Colab in the cloud).  One solution is to make use of a third party scheduling tool such as **Dask**. If pandas datatypes are specified the size is reduced to ~2.3GB in memory.  \n",
    "\n",
    "---\n",
    "\n",
    "## pip installs\n",
    "\n",
    "This notebook can be run using the `hds_code` environment.  Outside of this environment you will need to install `py7zr` using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVAvZragUWKC"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "  # This is the package that can extract .7z \n",
    "  !pip install py7zr==0.14.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caCpCpm7UDie"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IYrOh7etUCu3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# used for extracting .7z\n",
    "import py7zr\n",
    "\n",
    "# pythonic cross platform approach for HTTPS request\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTPxxA7kUS4g"
   },
   "source": [
    "## Helper functions\n",
    "\n",
    "We define the following helper functions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eT2-QQCYUBQP"
   },
   "outputs": [],
   "source": [
    "def as_dtype_dict(df):\n",
    "    '''\n",
    "    Converts a `DataFrame` of column names\n",
    "    and datatypes to a dict suitable for importing data.\n",
    "\n",
    "    Returns:\n",
    "    ------\n",
    "    dict\n",
    "    \n",
    "    {column_name: data_type}\n",
    "    '''\n",
    "    as_list = df.to_dict('split')['data']\n",
    "    return {i[0]:i[1] for i in as_list }\n",
    "\n",
    "\n",
    "def read_ed_data(data_types, archive, nhs_fname, clean_fname,\n",
    "                 verbose=True):\n",
    "    '''\n",
    "    Reads data in from the compressed file.\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    data_types: dict\n",
    "        dict linking column names to data types\n",
    "\n",
    "    archive: str\n",
    "      name of the archive file in .7z format\n",
    "\n",
    "    nhs_name: str\n",
    "        The NHS file name for the extracted file\n",
    "        \n",
    "    clean_name: str\n",
    "        A more usable and cleaned up filename if needed.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "    '''\n",
    "\n",
    "    # extract synthetic data preread into pandas\n",
    "    if verbose: print('extracting =>', end = ' ')\n",
    "    with py7zr.SevenZipFile(archive, mode='r') as z:\n",
    "        z.extractall()\n",
    "    if verbose: print('done')\n",
    "\n",
    "    # rename .7z file to cleaner format\n",
    "    os.rename(nhs_fname, clean_fname)\n",
    "\n",
    "    # read into pandas\n",
    "    if verbose: print('reading CSV =>', end = ' ')\n",
    "    df = pd.read_csv(clean_fname, dtype=data_types)\n",
    "    if verbose: print('done')\n",
    "\n",
    "    # cleans up uncompressed file as > 4.5GB in size.\n",
    "    os.remove(clean_fname)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def file_size(url):\n",
    "  '''\n",
    "  Returns remote file size in MB\n",
    "  '''\n",
    "  response = requests.head(url)\n",
    "  conversion = 1_000_000\n",
    "  fsize = float(response.headers['Content-Length']) / conversion\n",
    "  print(f'File size: {fsize:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YZO0JjQVTRz"
   },
   "source": [
    "---\n",
    "## Parameters\n",
    "\n",
    "All of the data used in this notebook is held remotely.  \n",
    "\n",
    "* The first url `DATA_URL` is the location of the compressed NHS Synthetic A&E dataset.\n",
    "* `META_URL` is a link to a Comma Separated Value (CSV) file in GitHub that contains meta data about the NHS dataset (user defined). There are two columns: field names and `pandas` data types. \n",
    "\n",
    "We also have some parameters to clean up:\n",
    "\n",
    "* `NHS_FNAME`: when decompressed the NHS file name contains both whitespace and the charactor '&'.  This is bad practice so we rename the file to:\n",
    "* `CLEAN_FNAME`: a more standard file name for the CSV.\n",
    "* `CLEAN_ARCHIVE_NAME`: when downloaded the arhive has an unusual file name and extension.  We rename this to make it easier to work with.\n",
    "* `CLEAN_UP_WHEN_DONE`: If true the download .7z file is deleted from the local machines harddrive when the analysis is complete.\n",
    "* `RESPONSE_SUCCESS`: a constant. The value 200 is returned if the dataset downloads successfully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_hIdEIFVWS3"
   },
   "outputs": [],
   "source": [
    "# NHS data url\n",
    "DATA_URL = 'https://nhsengland-direct-uploads.s3-eu-west-1.amazonaws.com/' \\\n",
    "             + 'A%26E+Synthetic+Data.7z?versionId=null'\n",
    "\n",
    "# meta data (created by me - contains pandas data types for columns)\n",
    "META_URL = 'https://raw.githubusercontent.com/health-data-science-OR/' \\\n",
    "                + 'hpdm139-datasets/main/synthetic_ed_meta_data.csv'\n",
    "\n",
    "# renaming info for file names\n",
    "NHS_FNAME = 'A&E Synthetic Data.csv'\n",
    "CLEAN_FNAME = 'ed_synthetic.csv'\n",
    "CLEAN_ARCHIVE_NAME = 'ed_synthetic.7z'\n",
    "\n",
    "# delete the downloaded file when done.\n",
    "CLEAN_UP_WHEN_DONE = True\n",
    "\n",
    "#download successful\n",
    "RESPONSE_SUCCESS = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGlV_TJiWCCS"
   },
   "source": [
    "---\n",
    "\n",
    "## Download code\n",
    "\n",
    "There are multiple ways to download the dataset.  Here we will request the file in python using the `requests` library.  This file is 630MB in size and download time will vary depending on your connection.\n",
    "\n",
    "Lets test and see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7A7diy_4V8t4"
   },
   "outputs": [],
   "source": [
    "# download file headers and report file size\n",
    "file_size(DATA_URL)\n",
    "\n",
    "# download the file...(only needs to be done once).\n",
    "print('downloading =>', end=' ')\n",
    "response = requests.get(DATA_URL)\n",
    "\n",
    "if response.status_code == RESPONSE_SUCCESS:\n",
    "    print('successful')\n",
    "\n",
    "    # write to file\n",
    "    with open(CLEAN_ARCHIVE_NAME, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "else:\n",
    "    print('file not found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lt9Kj8salCw9"
   },
   "source": [
    "---\n",
    "\n",
    "## Decompress .7z archive and read into `pandas.`\n",
    "\n",
    "The synthetic A&E dataset is a big file when extracted > 4.5GB and will take 5-7 minutes to read into pandas.  \n",
    "\n",
    "The function `read_ed_data` will first decompress the .7z file and then read it into a `DataFrame`.  The code automatically cleans up the decompressed file afterwards (deletes).  This is an optional step and if you need to re-read the data multiple times in your analysis code you may wish to keep it in its decompressed form until the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJTgre8yiMOw"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  '''\n",
    "  Read in the data, with specified column data types.\n",
    "\n",
    "  Returns:\n",
    "  -------\n",
    "  pd.DataFrame\n",
    "  '''\n",
    "  # read data types for each column of synthetic ED data\n",
    "  data_types = pd.read_csv(META_URL)\n",
    "\n",
    "  # extract and read\n",
    "  df = read_ed_data(as_dtype_dict(data_types), \n",
    "                    CLEAN_ARCHIVE_NAME, \n",
    "                    NHS_FNAME, CLEAN_FNAME)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5abBhnvpUIe"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2z97cFtpm5m"
   },
   "outputs": [],
   "source": [
    "# second not ignoring data types\n",
    "start_time = time.time()\n",
    "df = main()\n",
    "print(df.info())\n",
    "print(f'{time.time() - start_time}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZCjuot4H96p"
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHrA45qdz181"
   },
   "outputs": [],
   "source": [
    "# cleans up\n",
    "if CLEAN_UP_WHEN_DONE:\n",
    "  os.remove(CLEAN_ARCHIVE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yU2pkSWgH_j5"
   },
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nhs_synthetic_ed_download.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
